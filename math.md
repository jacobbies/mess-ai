MERT pretraining gives you expressive features from audio, but:

It’s not aligned specifically to interpretive nuance

It doesn't learn how a performance differs from a score, or from another interpretation

It treats everything as generic audio — not musicologically grounded representations

To unlock the artist × user × track interaction model, you need a space where:

A track's embedding reflects its expressive interpretation

A user's embedding reflects their expressive preferences

An artist’s embedding reflects their style across recordings

This comes from fine-tuning.

