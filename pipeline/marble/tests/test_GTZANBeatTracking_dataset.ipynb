{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac349c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "\n",
    "# If running inside \"tests\" folder, move up one level\n",
    "pwd = os.getcwd()\n",
    "if pwd.endswith(\"tests\"):\n",
    "    os.chdir(os.path.dirname(pwd))\n",
    "\n",
    "\n",
    "def visualize_and_play_with_onset_tone(\n",
    "    dataset,\n",
    "    beat_freq=440,\n",
    "    downbeat_freq=880,\n",
    "    tone_duration=0.05,\n",
    "    tone_amplitude=0.3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly select one clip from the dataset, visualize the original waveform and onset masks,\n",
    "    generate separate square-wave pulses for beats and downbeats at different frequencies,\n",
    "    overlay them onto the original audio, visualize the combined waveform and onset masks,\n",
    "    and display a frequency-domain plot of the combined signal. Finally, return an Audio\n",
    "    widget to play back the combined signal.\n",
    "\n",
    "    Args:\n",
    "        dataset: an instance of a GTZANBeatTracking dataset (inherited from _GTZANBeatTrackingAudioBase)\n",
    "        beat_freq: frequency (Hz) of the square-wave pulse for beats (default 440 Hz)\n",
    "        downbeat_freq: frequency (Hz) of the square-wave pulse for downbeats (default 880 Hz)\n",
    "        tone_duration: duration (seconds) of each square-wave pulse (default 0.05 s)\n",
    "        tone_amplitude: amplitude of each square-wave pulse (0.0 < amplitude <= 1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Randomly pick one clip index\n",
    "    idx = random.randrange(len(dataset))\n",
    "    waveform, label_dict, audio_path = dataset[idx]\n",
    "    # waveform: Tensor(shape=(channels, clip_len_samples))\n",
    "    # label_dict[\"beat\"]: Tensor(shape=(label_len,))\n",
    "    # label_dict.get(\"downbeat\"): Tensor(shape=(label_len,)) (if present)\n",
    "\n",
    "    # 2. Convert waveform to numpy and select the first channel if multi-channel\n",
    "    wav_np = waveform.cpu().numpy()\n",
    "    if wav_np.ndim > 1:\n",
    "        wav_np = wav_np[0, :]\n",
    "    clip_len = wav_np.shape[0]\n",
    "\n",
    "    fs = dataset.sample_rate           # sample rate (samples per second)\n",
    "    clip_seconds = dataset.clip_seconds  # clip duration in seconds\n",
    "\n",
    "    # Build time axis for waveform\n",
    "    t_wav = np.linspace(0, clip_seconds, num=clip_len, endpoint=False)\n",
    "\n",
    "    # 3. Extract beat and downbeat masks and compute their time arrays\n",
    "    beat_mask = label_dict[\"beat\"].cpu().numpy()  # shape = (label_len,)\n",
    "    db_mask = label_dict.get(\"downbeat\", None)\n",
    "    if db_mask is not None:\n",
    "        db_mask = db_mask.cpu().numpy()\n",
    "\n",
    "    label_len = beat_mask.shape[0]\n",
    "    label_freq = dataset.label_freq\n",
    "    # Time axis for masks: each frame i corresponds to time i / label_freq\n",
    "    t_mask = np.arange(label_len) / label_freq\n",
    "\n",
    "    # Compute beat times and downbeat times in seconds\n",
    "    beat_times = t_mask[beat_mask.astype(bool)]\n",
    "    if db_mask is not None:\n",
    "        db_times = t_mask[db_mask.astype(bool)]\n",
    "    else:\n",
    "        db_times = np.array([], dtype=float)\n",
    "\n",
    "    # 4. Prepare square-wave pulses for beat and downbeat separately\n",
    "    tone_len = int(round(tone_duration * fs))\n",
    "    if tone_len < 1:\n",
    "        tone_len = 1\n",
    "\n",
    "    # Time axis for a single pulse\n",
    "    t_tone = np.linspace(0, tone_duration, num=tone_len, endpoint=False)\n",
    "\n",
    "    # Square-wave pulse for beats at beat_freq\n",
    "    beat_pulse = tone_amplitude * np.sign(np.sin(2 * np.pi * beat_freq * t_tone))\n",
    "    beat_pulse = beat_pulse.astype(np.float32)\n",
    "\n",
    "    # Square-wave pulse for downbeats at downbeat_freq\n",
    "    db_pulse = tone_amplitude * np.sign(np.sin(2 * np.pi * downbeat_freq * t_tone))\n",
    "    db_pulse = db_pulse.astype(np.float32)\n",
    "\n",
    "    # 5. Create a combined signal by copying the original waveform\n",
    "    combined = wav_np.copy().astype(np.float32)\n",
    "\n",
    "    # Overlay beat pulses at each beat time\n",
    "    for bt in beat_times:\n",
    "        start_idx = int(round(bt * fs))\n",
    "        end_idx = start_idx + tone_len\n",
    "        if start_idx >= clip_len:\n",
    "            continue\n",
    "        if end_idx <= clip_len:\n",
    "            combined[start_idx:end_idx] += beat_pulse\n",
    "        else:\n",
    "            valid_len = clip_len - start_idx\n",
    "            combined[start_idx:clip_len] += beat_pulse[:valid_len]\n",
    "\n",
    "    # Overlay downbeat pulses at each downbeat time\n",
    "    for dt in db_times:\n",
    "        start_idx = int(round(dt * fs))\n",
    "        end_idx = start_idx + tone_len\n",
    "        if start_idx >= clip_len:\n",
    "            continue\n",
    "        if end_idx <= clip_len:\n",
    "            combined[start_idx:end_idx] += db_pulse\n",
    "        else:\n",
    "            valid_len = clip_len - start_idx\n",
    "            combined[start_idx:clip_len] += db_pulse[:valid_len]\n",
    "\n",
    "    # 6. Clip the combined signal to [-1.0, +1.0] to avoid distortion\n",
    "    combined = np.clip(combined, -1.0, +1.0)\n",
    "\n",
    "    # 7. Visualization\n",
    "    # Create a figure with three subplots:\n",
    "    #   1) Time-domain: original vs combined waveform with onset vertical lines\n",
    "    #   2) Time-domain: beat and downbeat masks (stem plots)\n",
    "    #   3) Frequency-domain: magnitude spectrum of the combined signal\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 10), constrained_layout=True)\n",
    "\n",
    "    # 7.1 Time-domain waveforms\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(t_wav, wav_np, color='gray', linewidth=1, label='Original Waveform')\n",
    "    ax1.plot(t_wav, combined, color='orange', linewidth=1, alpha=0.6, label='Combined Waveform')\n",
    "    ax1.set_xlim(0, clip_seconds)\n",
    "    ax1.set_ylabel(\"Amplitude\")\n",
    "    ax1.set_title(f\"Time-Domain Waveform with Onset Pulses\\n{audio_path} (clip idx={idx})\")\n",
    "\n",
    "    # Draw vertical lines for beat times (red dashed) and downbeat times (blue dash-dot)\n",
    "    if beat_times.size > 0:\n",
    "        for i, bt in enumerate(beat_times):\n",
    "            if i == 0:\n",
    "                ax1.axvline(bt, color='r', linestyle='--', alpha=0.7, label='Beat Onset')\n",
    "            else:\n",
    "                ax1.axvline(bt, color='r', linestyle='--', alpha=0.7)\n",
    "    if db_times.size > 0:\n",
    "        for i, dt in enumerate(db_times):\n",
    "            if i == 0:\n",
    "                ax1.axvline(dt, color='b', linestyle='-.', alpha=0.8, label='Downbeat Onset')\n",
    "            else:\n",
    "                ax1.axvline(dt, color='b', linestyle='-.', alpha=0.8)\n",
    "\n",
    "    ax1.legend(loc='upper right')\n",
    "\n",
    "    # 7.2 Onset masks (stem plots)\n",
    "    ax2 = axes[1]\n",
    "    markerline1, stemlines1, baseline1 = ax2.stem(\n",
    "        t_mask, beat_mask, linefmt='r-', markerfmt='ro', basefmt='k-', label='Beat Mask'\n",
    "    )\n",
    "    plt.setp(markerline1, markersize=4)\n",
    "    if db_mask is not None:\n",
    "        markerline2, stemlines2, baseline2 = ax2.stem(\n",
    "            t_mask, db_mask, linefmt='b-', markerfmt='bs', basefmt='k-', label='Downbeat Mask'\n",
    "        )\n",
    "        plt.setp(markerline2, markersize=4)\n",
    "    ax2.set_xlim(0, clip_seconds)\n",
    "    ax2.set_ylim(-0.1, 1.1)\n",
    "    ax2.set_xlabel(\"Time (s)\")\n",
    "    ax2.set_ylabel(\"Mask Value\")\n",
    "    ax2.set_title(\"Beat & Downbeat Onset Masks\")\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    # 7.3 Frequency-domain: magnitude spectrum of combined signal\n",
    "    ax3 = axes[2]\n",
    "    # Compute real FFT of the combined signal\n",
    "    fft_vals = np.fft.rfft(combined)\n",
    "    fft_freq = np.fft.rfftfreq(clip_len, d=1.0 / fs)\n",
    "    magnitude = np.abs(fft_vals)\n",
    "\n",
    "    # Plot magnitude spectrum (in linear scale)\n",
    "    ax3.plot(fft_freq, magnitude, color='purple', linewidth=0.8)\n",
    "    ax3.set_xlim(0, fs / 2)\n",
    "    ax3.set_xlabel(\"Frequency (Hz)\")\n",
    "    ax3.set_ylabel(\"Magnitude\")\n",
    "    ax3.set_title(\"Frequency-Domain (Magnitude Spectrum) of Combined Signal\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # 8. Return an Audio widget for playback of the combined signal\n",
    "    print(\"▶️ Playing the combined audio with onset-synchronized square-wave pulses:\")\n",
    "    return Audio(combined, rate=fs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dba9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marble.tasks.GTZANBeatTracking.datamodule import GTZANBeatTrackingAudioTrain\n",
    "\n",
    "# Instantiate the dataset (adjust paths as needed)\n",
    "dataset = GTZANBeatTrackingAudioTrain(\n",
    "    sample_rate=22050,\n",
    "    channels=1,\n",
    "    clip_seconds=10.0,\n",
    "    jsonl=\"data/GTZAN/GTZANBeatTracking.val.jsonl\",\n",
    "    label_freq=50,\n",
    "    num_neighbors=2,\n",
    "    channel_mode=\"mix\",\n",
    "    min_clip_ratio=0.5,\n",
    ")\n",
    "\n",
    "# Call the function; in a Jupyter environment the Audio widget will display automatically\n",
    "audio_widget = visualize_and_play_with_onset_tone(\n",
    "    dataset,\n",
    "    beat_freq=440,\n",
    "    downbeat_freq=880,\n",
    "    tone_duration=0.05,\n",
    "    tone_amplitude=0.3,\n",
    ")\n",
    "audio_widget  # Display the IPython Audio player in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87b48d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marble.tasks.GTZANBeatTracking.datamodule import GTZANBeatTrackingAudioTrain\n",
    "\n",
    "# Instantiate the dataset (adjust paths as needed)\n",
    "dataset = GTZANBeatTrackingAudioTrain(\n",
    "    sample_rate=22050,\n",
    "    channels=1,\n",
    "    clip_seconds=10.0,\n",
    "    jsonl=\"data/GTZAN/GTZANBeatTracking.val.jsonl\",\n",
    "    label_freq=75,\n",
    "    num_neighbors=2,\n",
    "    channel_mode=\"mix\",\n",
    "    min_clip_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c4a3d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos loss weight for beat: 13.78\n",
      "pos loss weight for downbeat: 57.40\n"
     ]
    }
   ],
   "source": [
    "total_num_pos_beat = 0\n",
    "total_num_neg_beat = 0\n",
    "total_num_pos_downbeat = 0\n",
    "total_num_neg_downbeat = 0\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    beat = dataset[idx][1]['beat']\n",
    "    downbeat = dataset[idx][1]['downbeat']\n",
    "\n",
    "    total_num_pos_beat += torch.sum(beat).item()\n",
    "    total_num_neg_beat += beat.shape[0] - torch.sum(beat).item()\n",
    "    total_num_pos_downbeat += torch.sum(downbeat).item()\n",
    "    total_num_neg_downbeat += downbeat.shape[0] - torch.sum(downbeat).item()\n",
    "\n",
    "loss_weight_beat = total_num_neg_beat / total_num_pos_beat\n",
    "loss_weight_downbeat = total_num_neg_downbeat / total_num_pos_downbeat\n",
    "print(f\"pos loss weight for beat: {loss_weight_beat:.2f}\")\n",
    "print(f\"pos loss weight for downbeat: {loss_weight_downbeat:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae85d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marble3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
